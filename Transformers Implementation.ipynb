{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf823976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fe8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, pe):\n",
    "    for i in range(0, 512, 2):\n",
    "        pe[0][1] = math.sin(pos /(10000**((2*i)/d_model)))\n",
    "        po[0][i+1] = math.cos(pos / (10000**((2*i)/d_model)))\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49a1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e924c4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x =np.array([[1.0, 0.0, 1.0, 0.0], # Input 1\n",
    "[0.0, 2.0, 0.0, 2.0], # Input 2\n",
    "[1.0, 1.0, 1.0, 1.0]]) # Input 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8f555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_model=4\n",
      "w_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
    "print(\"w_query\")\n",
    "w_query =np.array([[1, 0, 1],\n",
    "[1, 0, 0],\n",
    "[0, 0, 1],\n",
    "[0, 1, 1]])\n",
    "print(w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82f328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "[1, 1, 0],\n",
    "[0, 1, 0],\n",
    "[1, 1, 0]])\n",
    "print(w_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4460d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "[0, 3, 0],\n",
    "[1, 0, 3],\n",
    "[1, 1, 0]])\n",
    "print(w_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e101ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = [[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n",
      "K = [[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n",
      "V = [[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#finding out Q, K, V\n",
    "#dot product of weights and inputs\n",
    "Q = np.matmul(x, w_query)\n",
    "K = np.matmul(x, w_key)\n",
    "V = np.matmul(x, w_value)\n",
    "print(f'Q = {Q}')\n",
    "print(f'K = {K}')\n",
    "print(f'V = {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464c17c",
   "metadata": {},
   "source": [
    "--Scaled Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e24e9ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores is \n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "#square root of dim_k = 3 is 1....\n",
    "# rounded off as 1\n",
    "sqr_dim_k = 1\n",
    "attention_scores = (Q @ K.transpose())/sqr_dim_k\n",
    "print(f'Attention Scores is \\n{attention_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21ee95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a00ee5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "#scaled attention scores for each vector\n",
    "attention_scores[0] = softmax(attention_scores[0])\n",
    "attention_scores[1] = softmax(attention_scores[1])\n",
    "attention_scores[2] = softmax(attention_scores[2])\n",
    "for i in range(3):\n",
    "    print(attention_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3279afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n"
     ]
    }
   ],
   "source": [
    "#Values are \n",
    "for i in range(3):\n",
    "    print(V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd7c29",
   "metadata": {},
   "source": [
    "multiply the intermediate attention score by the 3 'value' vector one by one\n",
    "to zoom down into the inner workings of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69ecece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06337893833303763 ()\n",
      "0.4683105308334813 ()\n",
      "0.4683105308334813 ()\n"
     ]
    }
   ],
   "source": [
    "attention1 = attention_scores[0].reshape(-1, 1)\n",
    "for i in range(3):\n",
    "    print(attention1[i][0], attention1[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "997eee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06337894 0.12675788 0.19013681]\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "attention1 = attention_scores[0][0] *V[0]\n",
    "attention2 = attention_scores[0][1] *V[1]\n",
    "attention3 = attention_scores[0][2] *V[2]\n",
    "print(attention1)\n",
    "print(attention2)\n",
    "print(attention3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff24739",
   "metadata": {},
   "source": [
    "First line of the output values is the summation of 3 attention values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6332ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line of the output matrix is [1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "attention_input1 = attention1+attention2+attention3\n",
    "print(f'First line of the output matrix is {attention_input1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad61026",
   "metadata": {},
   "source": [
    "The second line will be for the output of the next input, i.e. input#2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27574a",
   "metadata": {},
   "source": [
    "Assuming random values for actual transformer original paper\n",
    "3 results of 64 dimensions each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c94effb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13373558 0.45790048 0.46243729 0.65822786 0.65960876 0.88452703\n",
      "  0.21171315 0.49923099 0.88647875 0.74942882 0.10166282 0.86643135\n",
      "  0.4000206  0.41588531 0.77027543 0.17391372 0.02864358 0.39789917\n",
      "  0.65566045 0.31968175 0.01715376 0.09230079 0.29778234 0.37813256\n",
      "  0.3480465  0.82773456 0.52386483 0.76657683 0.74222086 0.49702274\n",
      "  0.80666593 0.86689761 0.0079738  0.67573289 0.46455662 0.68468096\n",
      "  0.75690464 0.91015455 0.94766774 0.05978977 0.11233983 0.86417557\n",
      "  0.73286392 0.5383743  0.56802435 0.55843749 0.85841064 0.90572334\n",
      "  0.65563309 0.17282511 0.51676716 0.28526579 0.10489826 0.28224468\n",
      "  0.55616871 0.97049942 0.93734353 0.88992808 0.29995462 0.97489398\n",
      "  0.82127739 0.06546661 0.02773831 0.18601905]\n",
      " [0.02718355 0.4519143  0.93245817 0.62030322 0.26944405 0.09971777\n",
      "  0.14503153 0.37790984 0.54681393 0.76235591 0.43374949 0.85029703\n",
      "  0.20554719 0.05381368 0.86146385 0.23380015 0.35401261 0.26974306\n",
      "  0.26248555 0.58481752 0.08363527 0.72314849 0.99177062 0.59082474\n",
      "  0.95524309 0.85692063 0.29231157 0.83680823 0.35480342 0.94427551\n",
      "  0.68324198 0.69170295 0.17085461 0.53001789 0.72820301 0.71182762\n",
      "  0.89174057 0.62976791 0.93268877 0.01618592 0.99011388 0.82874868\n",
      "  0.88139182 0.03488156 0.60767451 0.31942911 0.08429579 0.07797886\n",
      "  0.09563717 0.39615819 0.21709917 0.46790802 0.03388578 0.7454617\n",
      "  0.78527304 0.22209311 0.5769664  0.81197892 0.30545755 0.70772521\n",
      "  0.35679822 0.79785237 0.54242057 0.69012092]\n",
      " [0.92731891 0.14471826 0.4567367  0.12591898 0.22900019 0.2651989\n",
      "  0.94068078 0.1581353  0.90346437 0.32662647 0.06928392 0.76204807\n",
      "  0.96597226 0.4203374  0.86337013 0.73706713 0.97006957 0.74110874\n",
      "  0.85556737 0.01197213 0.46765973 0.28098209 0.78396826 0.61537365\n",
      "  0.18333223 0.86988626 0.5543328  0.75893449 0.69277872 0.96185847\n",
      "  0.5131916  0.98210325 0.74882479 0.69971998 0.35085115 0.46160779\n",
      "  0.62057253 0.63566561 0.16197429 0.82161742 0.21938484 0.52696816\n",
      "  0.78052797 0.59766838 0.13056264 0.97623003 0.00297629 0.55774242\n",
      "  0.22488943 0.930598   0.36637244 0.29529861 0.47454122 0.91916004\n",
      "  0.81317244 0.38678253 0.78644732 0.46065859 0.80063315 0.27136751\n",
      "  0.88355835 0.52566744 0.47130558 0.82317911]]\n"
     ]
    }
   ],
   "source": [
    "attention_head1 = np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b71b2c",
   "metadata": {},
   "source": [
    "Assumption of 8 heads of the attention sub-layer \n",
    "so 3 input vectors has 3 output vectors of d_model=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5dbb0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained heads of the attention layer are:\n",
      "shape of one head (3, 64) dimension of 8 heads 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained heads of the attention layer are:\")\n",
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4300dd",
   "metadata": {},
   "source": [
    "Concatenation of the output of the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9412a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73002511 0.3502825  0.70160739 ... 0.59884761 0.33574044 0.39792195]\n",
      " [0.20490409 0.32669716 0.53695721 ... 0.13376205 0.94136011 0.90947728]\n",
      " [0.20792747 0.91936149 0.47502873 ... 0.61196262 0.67103404 0.99593312]] (3, 512)\n"
     ]
    }
   ],
   "source": [
    "output_attention = np.hstack((z0h1, z1h2, z2h3, z3h4, z4h5, z5h6, z6h7, z7h8))\n",
    "print(output_attention, output_attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b218fbf",
   "metadata": {},
   "source": [
    "POST-LAYER NORMALIZATION\n",
    "LayerNorm(x+Sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87b62f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c12e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a50afa644c776f7a2a60a52e60e30bd38a1de8f1520052de409d0e9c4f2415b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
